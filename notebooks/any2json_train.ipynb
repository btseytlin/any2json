{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "m2JXjC0EdF8R"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV6xN2PEZmR5"
      },
      "source": [
        "# Load secrets and prepare gdrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hP5C05m6ZoRw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1pTSM6LFNyH",
        "outputId": "bda75cef-ce2b-41de-b179-d9f8d931d552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BapJg0Bjh1GG",
        "outputId": "3a8fa766-6364-451e-a964-cfffccbd8924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=os.environ['WANDB_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PohmO8t-Y0pL"
      },
      "source": [
        "# Check out repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxY-qZB3Yj2a",
        "outputId": "373e21f0-6f32-43e7-e3e0-b37e485e2352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'any2json'...\n",
            "remote: Enumerating objects: 520, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 520 (delta 108), reused 100 (delta 50), pack-reused 362 (from 1)\u001b[K\n",
            "Receiving objects: 100% (520/520), 47.42 KiB | 408.00 KiB/s, done.\n",
            "Resolving deltas: 100% (289/289), done.\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 51 (delta 1), reused 5 (delta 0), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (51/51), 259.98 KiB | 1.31 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "HEAD is now at 5747717 Vibin\n"
          ]
        }
      ],
      "source": [
        "!rm -r any2json\n",
        "!git clone --filter=blob:none  https://github.com/btseytlin/any2json.git\n",
        "!cd any2json && git fetch && git reset --hard origin/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "_6Mymqpgf120"
      },
      "outputs": [],
      "source": [
        "!cd any2json && uv -q pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgKzsBnCZW4z"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkjo630oYpCj",
        "outputId": "807db2ce-1ee0-472f-98bd-fe19306a3086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size 183663\n",
            "Test size 4959\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_data', 'schema', 'output', 'meta'],\n",
              "    num_rows: 183663\n",
              "})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_ID = 'btseytlin/any2json'\n",
        "\n",
        "dataset = load_dataset(DATASET_ID)\n",
        "\n",
        "print(\"Train size\", len(dataset['train']))\n",
        "print(\"Test size\", len(dataset['test']))\n",
        "dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NVJh7w2ei3o"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fLDujjeHwEjc",
        "outputId": "b496827d-ab21-4d10-db4a-d73c38f27c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-11 15:48:04.530067: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-11 15:48:04.548930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754927284.570480   10599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754927284.577072   10599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754927284.594391   10599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754927284.594416   10599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754927284.594419   10599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754927284.594421   10599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-11 15:48:04.599207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:any2json:Configured any2json logger with level INFO\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbtseytlin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250811_154809-pvhuib5p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtwilight-serenity-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/btseytlin/any2json\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/btseytlin/any2json/runs/pvhuib5p\u001b[0m\n",
            "INFO:any2json:Target test size: 5\n",
            "INFO:any2json:Train group sizes (top 10): [(131, 1), (233, 1), (5945, 1), (1238, 1), (5946, 1), (80, 1), (5947, 1), (5948, 1), (1239, 1), (5949, 1)]\n",
            "INFO:any2json:Test group sizes (top 10): [(516, 1), (3585, 1), (3590, 1), (23222, 1), (5978, 1)]\n",
            "Filter:   0% 0/5 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1399 > 512). Running this sequence through the model will result in indexing errors\n",
            "Filter: 100% 5/5 [00:00<00:00, 371.55 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 396.22 examples/s]\n",
            "/content/any2json/any2json/training/train.py:351: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'loss': 1.104, 'grad_norm': 0.1959623545408249, 'learning_rate': 2.6288659793814435e-05, 'epoch': 5.0}\n",
            " 50% 50/100 [01:14<01:01,  1.23s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8574655652046204, 'eval_runtime': 0.8908, 'eval_samples_per_second': 5.613, 'eval_steps_per_second': 5.613, 'epoch': 5.0}\n",
            " 50% 50/100 [01:15<01:01,  1.23s/it]\n",
            "100% 5/5 [00:00<00:00, 30.81it/s]\u001b[A\n",
            "{'loss': 0.7968, 'grad_norm': 0.3193979561328888, 'learning_rate': 5.154639175257732e-07, 'epoch': 10.0}\n",
            "100% 100/100 [02:31<00:00,  1.15s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7898894548416138, 'eval_runtime': 0.8865, 'eval_samples_per_second': 5.64, 'eval_steps_per_second': 5.64, 'epoch': 10.0}\n",
            "100% 100/100 [02:32<00:00,  1.15s/it]\n",
            "100% 5/5 [00:00<00:00, 31.68it/s]\u001b[A\n",
            "                                 \u001b[AThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
            "{'train_runtime': 156.3692, 'train_samples_per_second': 5.82, 'train_steps_per_second': 0.64, 'train_loss': 0.950374984741211, 'epoch': 10.0}\n",
            "100% 100/100 [02:36<00:00,  1.56s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mtwilight-serenity-21\u001b[0m at: \u001b[34mhttps://wandb.ai/btseytlin/any2json/runs/pvhuib5p\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250811_154809-pvhuib5p/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "!python any2json/any2json/training/train.py train --bf16 --per-device-train-batch-size 1 --gradient-accumulation-steps 10 --debug-limit 100 --num-train-epochs=10  --max-source-length=2048 --max-target-length=2048 --logging-steps 50 --eval-steps 50 --save-steps 100000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXL5-0nMVI1"
      },
      "source": [
        "# Upload results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "2ycwIQUyB7eY"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import gzip\n",
        "# from datetime import datetime\n",
        "\n",
        "# dt = datetime.now().isoformat()\n",
        "\n",
        "# # upload benchmark results to gdrive\n",
        "\n",
        "# out_path = f\"/content/drive/MyDrive/any2json/benchmarks/\"\n",
        "# os.makedirs(out_path, exist_ok=True)\n",
        "\n",
        "# zip_path = f\"/content/benchmark_results_{dt}\"\n",
        "# shutil.make_archive(zip_path, \"zip\", \"/content/benchmark_results\")\n",
        "\n",
        "# shutil.copy(f\"{zip_path}.zip\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOwDdbHwFbon"
      },
      "source": [
        "# Shutdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "aaXZA1M95boc"
      },
      "outputs": [],
      "source": [
        "# # shutdown instance\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "XhRrFf_LPu1Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06de8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a931b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boris/Documents/any2json/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:any2json:Configured any2json logger with level INFO\n",
      "INFO:any2json:Applied debug limit: 100, now 100 train samples\n",
      "INFO:any2json:Loaded 100 train samples\n",
      "INFO:any2json:Minifying JSON schemas and outputs on load\n",
      "Map (num_proc=4): 100%|██████████| 100/100 [00:00<00:00, 727.09 examples/s]\n",
      "INFO:any2json:Target test size: 20\n",
      "INFO:any2json:Train group sizes (top 10): [(0, 7), (3697, 1), (375, 1), (179, 1), (8045, 1), (11951, 1), (11952, 1), (1578, 1), (11954, 1), (161, 1)]\n",
      "INFO:any2json:Test group sizes (top 10): [(11950, 1), (3338, 1), (11956, 1), (4444, 1), (6985, 1), (8046, 1), (11967, 1), (3339, 1), (1929, 1), (22228, 1)]\n",
      "INFO:any2json:Prepared splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_data', 'schema', 'output', 'meta'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_data', 'schema', 'output', 'meta'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "INFO:any2json:Preparing train dataset\n",
      "Flattening the indices: 100%|██████████| 80/80 [00:00<00:00, 24279.62 examples/s]\n",
      "Parameter 'function'=<function AugmentTokenizeDataset.from_raw_dataset.<locals>.tokenize_with_idx at 0x327a75120> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function AugmentTokenizeDataset.from_raw_dataset.<locals>.tokenize_with_idx at 0x327a75120> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=4): 100%|██████████| 80/80 [00:01<00:00, 42.59 examples/s]\n",
      "Filter (num_proc=4): 100%|██████████| 80/80 [00:00<00:00, 958.34 examples/s]\n",
      "INFO:any2json:Preparing validation dataset\n",
      "Map (num_proc=4): 100%|██████████| 20/20 [00:01<00:00, 11.26 examples/s]\n",
      "Filter (num_proc=4): 100%|██████████| 20/20 [00:00<00:00, 271.26 examples/s]\n",
      "INFO:any2json:Prepared datasets\n",
      "/Users/boris/Documents/any2json/any2json/training/train.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from any2json.training.augment import Augmentor\n",
    "from any2json.utils import configure_loggers, logger, try_minify_json_string\n",
    "from any2json.training.train import prepare_model_and_tokenizer, PipelineConfig, create_trainer, prepare_dataset\n",
    "from any2json.training.utils import (\n",
    "    build_tokenized_length_filter_fn,\n",
    "    load_hf_dataset,\n",
    "    apply_debug_limit,\n",
    "    build_tokenize_fn,\n",
    "    CausalLMDataCollator,\n",
    "    prepare_splits,\n",
    "    process_raw_to_tokenized,\n",
    ")\n",
    "from any2json.training.dataset import AugmentTokenizeDataset\n",
    "\n",
    "# Configure logging\n",
    "configure_loggers(level=\"INFO\", basic_level=\"WARNING\")\n",
    "\n",
    "# Default model and config\n",
    "DEFAULT_MODEL = \"google/gemma-3-270m\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DebugConfig(PipelineConfig):\n",
    "    dataset_path: str = \"btseytlin/any2json\"\n",
    "    model_name: str = DEFAULT_MODEL\n",
    "    max_sequence_length: int = 2560\n",
    "    debug_limit: int = 100  # Small for debugging\n",
    "    val_size: int = 20\n",
    "    dataloader_num_proc: int = 4\n",
    "    augment: bool = True\n",
    "    seed: int = 42\n",
    "    pad_to_multiple_of: int = 8\n",
    "    debug_tokens: bool = True\n",
    "\n",
    "\n",
    "def setup_debug_environment():\n",
    "    \"\"\"Setup the exact same environment as training\"\"\"\n",
    "    cfg = DebugConfig()\n",
    "\n",
    "    args = TrainingArguments()\n",
    "\n",
    "    # Load tokenizer\n",
    "    model, tokenizer = prepare_model_and_tokenizer(cfg, args)\n",
    "\n",
    "    train_dataset, eval_dataset = prepare_dataset(cfg, args, tokenizer)\n",
    "    trainer = create_trainer(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        pad_to_multiple_of=cfg.pad_to_multiple_of,\n",
    "        debug_tokens=cfg.debug_tokens,\n",
    "        max_sequence_length=cfg.max_sequence_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"config\": cfg,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"val_dataset\": eval_dataset,\n",
    "        \"collator\": trainer.data_collator,\n",
    "    }\n",
    "\n",
    "\n",
    "# Helper functions for debugging\n",
    "def print_tokenized_example(tokenizer, example, title=\"Example\"):\n",
    "    \"\"\"Print a tokenized example in detail\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "\n",
    "    input_ids = example.get(\"input_ids\", [])\n",
    "    labels = example.get(\"labels\", [])\n",
    "\n",
    "    print(f\"Input IDs length: {len(input_ids)}\")\n",
    "    print(f\"Labels length: {len(labels)}\")\n",
    "\n",
    "    if input_ids:\n",
    "        # Decode the full sequence\n",
    "        full_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(f\"Full decoded text:\\n{repr(full_text)}\")\n",
    "\n",
    "        # Find the boundary between prompt and target\n",
    "        if labels:\n",
    "            try:\n",
    "                start_idx = next(i for i, l in enumerate(labels) if l != -100)\n",
    "                prompt_ids = input_ids[:start_idx]\n",
    "                target_ids = [\n",
    "                    input_ids[i]\n",
    "                    for i in range(len(input_ids))\n",
    "                    if i < len(labels) and labels[i] != -100\n",
    "                ]\n",
    "\n",
    "                prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
    "                target_text = tokenizer.decode(target_ids, skip_special_tokens=False)\n",
    "\n",
    "                print(f\"\\nPrompt ({len(prompt_ids)} tokens):\\n{repr(prompt_text)}\")\n",
    "                print(f\"\\nTarget ({len(target_ids)} tokens):\\n{repr(target_text)}\")\n",
    "                print(f\"\\nBoundary index: {start_idx}\")\n",
    "\n",
    "            except StopIteration:\n",
    "                print(\"❌ NO NON-MASKED LABELS FOUND!\")\n",
    "                print(f\"Labels sample: {labels[:20]}...\")\n",
    "\n",
    "    print(\n",
    "        f\"Input IDs: {input_ids[:10]}...{input_ids[-5:] if len(input_ids) > 15 else ''}\"\n",
    "    )\n",
    "    print(f\"Labels:    {labels[:10]}...{labels[-5:] if len(labels) > 15 else ''}\")\n",
    "\n",
    "\n",
    "def debug_collator_batch(collator, examples, tokenizer):\n",
    "    \"\"\"Debug what happens during collation\"\"\"\n",
    "    print(f\"\\n=== COLLATOR DEBUG ===\")\n",
    "    print(f\"Input batch size: {len(examples)}\")\n",
    "\n",
    "    for i, ex in enumerate(examples):\n",
    "        print(\n",
    "            f\"Example {i}: {len(ex['input_ids'])} tokens, {sum(1 for l in ex['labels'] if l != -100)} target tokens\"\n",
    "        )\n",
    "\n",
    "    # Apply collator\n",
    "    batch = collator(examples)\n",
    "\n",
    "    print(f\"\\nAfter collation:\")\n",
    "    print(\n",
    "        f\"Batch shapes: input_ids={batch['input_ids'].shape}, labels={batch['labels'].shape}\"\n",
    "    )\n",
    "\n",
    "    # Check each example in the batch\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        input_ids = batch[\"input_ids\"][i]\n",
    "        labels = batch[\"labels\"][i]\n",
    "        attention_mask = batch[\"attention_mask\"][i]\n",
    "\n",
    "        non_pad_count = sum(\n",
    "            1 for t in input_ids.tolist() if t != tokenizer.pad_token_id\n",
    "        )\n",
    "        non_masked_count = sum(1 for l in labels.tolist() if l != -100)\n",
    "        attention_count = sum(attention_mask.tolist())\n",
    "\n",
    "        assert attention_count == non_pad_count\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        print(\n",
    "            f\"  Example {i}: {non_pad_count} non-pad, {non_masked_count} non-masked, {attention_count} attention\"\n",
    "        )\n",
    "\n",
    "        if non_pad_count == 0:\n",
    "            print(f\"    ❌ ALL PADDING! This is the bug!\")\n",
    "        elif non_masked_count == 0:\n",
    "            print(f\"    ❌ ALL LABELS MASKED! This causes eval issues!\")\n",
    "\n",
    "        # Show the actual tokens for debugging\n",
    "        if non_pad_count > 0:\n",
    "            decoded = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "            print(f\"    Decoded : {repr(decoded)}\")\n",
    "\n",
    "\n",
    "debug_data = setup_debug_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38d218a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['config', 'tokenizer', 'train_dataset', 'val_dataset', 'collator'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8ede6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 78\n",
      "Val dataset size: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(debug_data['train_dataset'])}\")\n",
    "print(f\"Val dataset size: {len(debug_data['val_dataset'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1144fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 808.14it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 2259.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(debug_data[\"train_dataset\"]):\n",
    "    pass\n",
    "\n",
    "for _ in tqdm(debug_data[\"val_dataset\"]):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "569e771b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing train examples ---\n",
      "\n",
      "=== Train Example 39 ===\n",
      "Input IDs length: 391\n",
      "Labels length: 391\n",
      "Full decoded text:\n",
      "'<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"also_interesting_story_office_town\":{\"type\":[\"integer\",\"null\"]},\"build_four_late_song_while\":{\"type\":[\"string\",\"null\"]},\"explain_join_within\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]\\n    INSERT INTO `leader` (\\n        explain_join_within,\\n        also_int\\'eresting_story_office_town,\\n        build_four_lat e_son_g_while)\\n     VALUES\\n(\\'104.181.251.246\\', -23378, \\'9.8\\'),\\n(\\'182.120.82.44\\', 849006, \\'56.9\\'),\\n(\\'204.240.33.9-4\\', 893876, \\'722.23\\');[OUTPUT][{\"also_interesting_story_office_town\":-23378,\"build_four_late_song_while\":\"9.8\",\"explain_join_within\":\"104.181.251.246\"},{\"also_interesting_story_office_town\":849006,\"build_four_late_song_while\":\"56.9\",\"explain_join_within\":\"182.120.82.44\"},{\"also_interesting_story_office_town\":893876,\"build_four_late_song_while\":\"722.23\",\"explain_join_within\":\"204.240.33.94\"}]<eos>'\n",
      "\n",
      "Prompt (228 tokens):\n",
      "'<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"also_interesting_story_office_town\":{\"type\":[\"integer\",\"null\"]},\"build_four_late_song_while\":{\"type\":[\"string\",\"null\"]},\"explain_join_within\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]\\n    INSERT INTO `leader` (\\n        explain_join_within,\\n        also_int\\'eresting_story_office_town,\\n        build_four_lat e_son_g_while)\\n     VALUES\\n(\\'104.181.251.246\\', -23378, \\'9.8\\'),\\n(\\'182.120.82.44\\', 849006, \\'56.9\\'),\\n(\\'204.240.33.9-4\\', 893876, \\'722.23\\');[OUTPUT]'\n",
      "\n",
      "Target (163 tokens):\n",
      "'[{\"also_interesting_story_office_town\":-23378,\"build_four_late_song_while\":\"9.8\",\"explain_join_within\":\"104.181.251.246\"},{\"also_interesting_story_office_town\":849006,\"build_four_late_song_while\":\"56.9\",\"explain_join_within\":\"182.120.82.44\"},{\"also_interesting_story_office_town\":893876,\"build_four_late_song_while\":\"722.23\",\"explain_join_within\":\"204.240.33.94\"}]<eos>'\n",
      "\n",
      "Boundary index: 228\n",
      "Input IDs: [2, 34339, 2744, 1262, 531, 8373, 3894, 531, 10434, 15740]...[236819, 236812, 236775, 15947, 1]\n",
      "Labels:    [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]...[236819, 236812, 236775, 15947, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing train examples ---\")\n",
    "\n",
    "i = random.randint(0, len(debug_data[\"train_dataset\"]) - 1)\n",
    "i = 39\n",
    "example = debug_data[\"train_dataset\"][i]\n",
    "print_tokenized_example(debug_data[\"tokenizer\"], example, f\"Train Example {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f2295d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing val examples ---\n",
      "\n",
      "=== Val Example 1 ===\n",
      "Input IDs length: 159\n",
      "Labels length: 159\n",
      "Full decoded text:\n",
      "'<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"properties\":{\"HelpThose\":{\"type\":[\"string\",\"null\"]},\"SaveCourseVariousTechnology\":{\"type\":[\"string\",\"null\"]},\"company_former_dark_peace_account\":{\"type\":[\"integer\",\"null\"]}},\"type\":[\"object\",\"null\"]}[INPUT]\\n    INSERT INTO `we` (\\n        HelpThose,\\n        company_former_dark_peace_account,\\n        SaveCourseVariousTechnology)\\n     VALUES\\n(\\'-80\\', 182716, \\'60.4\\');[OUTPUT]{\"HelpThose\":\"-80\",\"SaveCourseVariousTechnology\":\"60.4\",\"company_former_dark_peace_account\":182716}<eos>'\n",
      "\n",
      "Prompt (123 tokens):\n",
      "'<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"properties\":{\"HelpThose\":{\"type\":[\"string\",\"null\"]},\"SaveCourseVariousTechnology\":{\"type\":[\"string\",\"null\"]},\"company_former_dark_peace_account\":{\"type\":[\"integer\",\"null\"]}},\"type\":[\"object\",\"null\"]}[INPUT]\\n    INSERT INTO `we` (\\n        HelpThose,\\n        company_former_dark_peace_account,\\n        SaveCourseVariousTechnology)\\n     VALUES\\n(\\'-80\\', 182716, \\'60.4\\');[OUTPUT]'\n",
      "\n",
      "Target (36 tokens):\n",
      "'{\"HelpThose\":\"-80\",\"SaveCourseVariousTechnology\":\"60.4\",\"company_former_dark_peace_account\":182716}<eos>'\n",
      "\n",
      "Boundary index: 123\n",
      "Input IDs: [2, 34339, 2744, 1262, 531, 8373, 3894, 531, 10434, 15740]...[236832, 236770, 236825, 236783, 1]\n",
      "Labels:    [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]...[236832, 236770, 236825, 236783, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing val examples ---\")\n",
    "\n",
    "i = random.randint(0, len(debug_data[\"val_dataset\"]) - 1)\n",
    "i = 1\n",
    "example = debug_data[\"val_dataset\"][i]\n",
    "print_tokenized_example(debug_data[\"tokenizer\"], example, f\"Val Example {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "39ce606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing collation ---\n",
      "\n",
      "=== COLLATOR DEBUG ===\n",
      "Input batch size: 3\n",
      "Example 0: 656 tokens, 248 target tokens\n",
      "Example 1: 159 tokens, 36 target tokens\n",
      "Example 2: 1998 tokens, 923 target tokens\n",
      "\n",
      "After collation:\n",
      "Batch shapes: input_ids=torch.Size([3, 2000]), labels=torch.Size([3, 2000])\n",
      "  Example 0: 656 non-pad, 248 non-masked, 656 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"AgreeMyWould\":{\"type\":[\"string\",\"null\"]},\"GroupLaterWhether\":{\"type\":[\"string\",\"null\"]},\"HitUnit\":{\"type\":[\"integer\",\"null\"]},\"LikeMiddleThis\":{\"type\":[\"string\",\"null\"]},\"PositionFormTogetherMakeYet\":{\"type\":[\"string\",\"null\"]},\"RemainDevelopmentSomebody\":{\"type\":[\"string\",\"null\"]},\"TalkAHit\":{\"type\":[\"number\",\"null\"]},\"add_such_allow_able\":{\"type\":[\"string\",\"null\"]},\"old_door_become_wear_spring\":{\"type\":[\"string\",\"null\"]},\"respond\":{\"type\":[\"string\",\"null\"]},\"several_whatever_talk_fly\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]\\n    INSERT INTO `hard` (\\n        add_such_allow_able,\\n        AgreeMyWould,\\n        GroupLaterWhether,\\n        RemainDevelopmentSomebody,\\n        HitUnit,\\n        several_whatever_talk_fly,\\n        PositionFormTogetherMakeYet,\\n        TalkAHit,\\n        respond,\\n        LikeMiddleThis,\\n        old_door_become_wear_spring)\\n     VALUES\\n(\\'87.14.87.170\\', \\'smithbelinda@example.com\\', None, \\'HKD\\', -84, \\'-881.78\\', None, 27.158, \\'7293021912\\', \\'59\\', \\'-58\\'),\\n(\\'160.151.170.110\\', \\'martinezkathleen@example.net\\', None, \\'LKR\\', 38, \\'-54.1\\', None, 27.4, \\'2507480677\\', \\'-81\\', \\'97\\');[OUTPUT][{\"AgreeMyWould\":\"smithbelinda@example.com\",\"GroupLaterWhether\":null,\"HitUnit\":-84,\"LikeMiddleThis\":\"59\",\"PositionFormTogetherMakeYet\":null,\"RemainDevelopmentSomebody\":\"HKD\",\"TalkAHit\":27.158,\"add_such_allow_able\":\"87.14.87.170\",\"old_door_become_wear_spring\":\"-58\",\"respond\":\"7293021912\",\"several_whatever_talk_fly\":\"-881.78\"},{\"AgreeMyWould\":\"martinezkathleen@example.net\",\"GroupLaterWhether\":null,\"HitUnit\":38,\"LikeMiddleThis\":\"-81\",\"PositionFormTogetherMakeYet\":null,\"RemainDevelopmentSomebody\":\"LKR\",\"TalkAHit\":27.4,\"add_such_allow_able\":\"160.151.170.110\",\"old_door_become_wear_spring\":\"97\",\"respond\":\"2507480677\",\"several_whatever_talk_fly\":\"-54.1\"}]<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Example 1: 159 non-pad, 36 non-masked, 159 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"properties\":{\"HelpThose\":{\"type\":[\"string\",\"null\"]},\"SaveCourseVariousTechnology\":{\"type\":[\"string\",\"null\"]},\"company_former_dark_peace_account\":{\"type\":[\"integer\",\"null\"]}},\"type\":[\"object\",\"null\"]}[INPUT]\\n    INSERT INTO `we` (\\n        HelpThose,\\n        company_former_dark_peace_account,\\n        SaveCourseVariousTechnology)\\n     VALUES\\n(\\'-80\\', 182716, \\'60.4\\');[OUTPUT]{\"HelpThose\":\"-80\",\"SaveCourseVariousTechnology\":\"60.4\",\"company_former_dark_peace_account\":182716}<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Example 2: 1998 non-pad, 923 non-masked, 1998 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"BuildingSend\":{\"type\":[\"string\",\"null\"]},\"EachHistoryHitMachine\":{\"type\":[\"string\",\"null\"]},\"OnManagerAttorneyAirBlood\":{\"type\":[\"number\",\"null\"]},\"Trade\":{\"type\":[\"string\",\"null\"]},\"TreatmentAnimalCertainlyMillion\":{\"type\":[\"string\",\"null\"]},\"Whether\":{\"type\":[\"string\",\"null\"]},\"against_page_prevent_bring\":{\"type\":[\"string\",\"null\"]},\"arrive_owner_last\":{\"type\":[\"string\",\"null\"]},\"artist\":{\"type\":[\"string\",\"null\"]},\"book_lose_memory_hotel\":{\"type\":[\"string\",\"null\"]},\"book_special_goal_order\":{\"type\":[\"string\",\"null\"]},\"dog_writer_step_our_certain\":{\"type\":[\"string\",\"null\"]},\"everything_clear\":{\"type\":[\"string\",\"null\"]},\"food_cultural\":{\"type\":[\"string\",\"null\"]},\"history_now_scene_movement_per\":{\"type\":[\"string\",\"null\"]},\"large_growth_little_bit\":{\"type\":[\"string\",\"null\"]},\"major_continue_follow_bill_election\":{\"type\":[\"string\",\"null\"]},\"my_laugh\":{\"type\":[\"string\",\"null\"]},\"sport_must_interest\":{\"type\":[\"boolean\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]\\n    INSERT INTO `shoulder` (\\n        major_continue_follow_bill_election,\\n        large_growth_little_bit,\\n        against_page_prevent_bring,\\n        TreatmentAnimalCertainlyMillion,\\n        everything_clear,\\n        dog_writer_step_our_certain,\\n        book_special_goal_order,\\n        BuildingSend,\\n        EachHistoryHitMachine,\\n        history_now_scene_movement_per,\\n        artist,\\n        my_laugh,\\n        arrive_owner_last,\\n        sport_must_interest,\\n        Whether,\\n        food_cultural,\\n        OnManagerAttorneyAirBlood,\\n        book_lose_memory_hotel,\\n        Trade)\\n     VALUES\\n(\\'Susan Bowman\\', \\'Unit 3392 Box 5238\\\\nDPO AA 43404\\', \\'Yemeni rial\\', \\'http://best.info/\\', \\'THB\\', \\'23.75.209.172\\', \\'2023-01-09\\', \\'4.83\\', \\'180.88.57.249\\', \\'2023-01-09\\', \\'2023-01-09\\', \\'-41\\', \\'2023-01-09\\', True, \\'Doctor almost their recently.\\', \\'9c5b:9de:4709:1ff3:3bed:f194:46b:87b2\\', None, \\'204.138.48.0\\', \\'1984-03-03\\'),\\n(\\'Jordan Evans\\', \\'81701 Jones Ridge Apt. 121\\\\nLake Debra, AS 58015\\', \\'Zambian kwacha\\', \\'http://hicks.com/\\', \\'IRR\\', \\'206.68.67.122\\', \\'2023-01-09\\', \\'243.0\\', \\'109.180.93.77\\', \\'2023-01-09\\', \\'2023-01-09\\', \\'-4\\', \\'2023-01-09\\', True, \\'Fast federal body.\\', \\'a08a:f147:be81:2ca4:bcc7:fea5:8e6a:2aed\\', None, \\'209.238.114.191\\', \\'1995-11-25\\'),\\n(\\'Susan Townsend\\', \\'5739 Luis Ridge Suite 894\\\\nOdomfurt, CO 46210\\', \\'Australian dollar\\', \\'http://arellano.com/\\', \\'NAD\\', \\'143.96.139.141\\', \\'2023-01-09\\', \\'90.178\\', \\'130.74.208.110\\', \\'2023-01-09\\', \\'2023-01-09\\', \\'-71\\', \\'2023-01-09\\', False, \\'Far they brother eat cultural.\\', \\'c6a7:df6:6912:5e61:c07f:6099:a4e8:4783\\', None, \\'1.191.245.170\\', \\'2004-08-12\\');[OUTPUT][{\"BuildingSend\":\"4.83\",\"EachHistoryHitMachine\":\"180.88.57.249\",\"OnManagerAttorneyAirBlood\":null,\"Trade\":\"1984-03-03\",\"TreatmentAnimalCertainlyMillion\":\"http://best.info/\",\"Whether\":\"Doctor almost their recently.\",\"against_page_prevent_bring\":\"Yemeni rial\",\"arrive_owner_last\":\"2023-01-09\",\"artist\":\"2023-01-09\",\"book_lose_memory_hotel\":\"204.138.48.0\",\"book_special_goal_order\":\"2023-01-09\",\"dog_writer_step_our_certain\":\"23.75.209.172\",\"everything_clear\":\"THB\",\"food_cultural\":\"9c5b:9de:4709:1ff3:3bed:f194:46b:87b2\",\"history_now_scene_movement_per\":\"2023-01-09\",\"large_growth_little_bit\":\"Unit 3392 Box 5238\\\\nDPO AA 43404\",\"major_continue_follow_bill_election\":\"Susan Bowman\",\"my_laugh\":\"-41\",\"sport_must_interest\":true},{\"BuildingSend\":\"243.0\",\"EachHistoryHitMachine\":\"109.180.93.77\",\"OnManagerAttorneyAirBlood\":null,\"Trade\":\"1995-11-25\",\"TreatmentAnimalCertainlyMillion\":\"http://hicks.com/\",\"Whether\":\"Fast federal body.\",\"against_page_prevent_bring\":\"Zambian kwacha\",\"arrive_owner_last\":\"2023-01-09\",\"artist\":\"2023-01-09\",\"book_lose_memory_hotel\":\"209.238.114.191\",\"book_special_goal_order\":\"2023-01-09\",\"dog_writer_step_our_certain\":\"206.68.67.122\",\"everything_clear\":\"IRR\",\"food_cultural\":\"a08a:f147:be81:2ca4:bcc7:fea5:8e6a:2aed\",\"history_now_scene_movement_per\":\"2023-01-09\",\"large_growth_little_bit\":\"81701 Jones Ridge Apt. 121\\\\nLake Debra, AS 58015\",\"major_continue_follow_bill_election\":\"Jordan Evans\",\"my_laugh\":\"-4\",\"sport_must_interest\":true},{\"BuildingSend\":\"90.178\",\"EachHistoryHitMachine\":\"130.74.208.110\",\"OnManagerAttorneyAirBlood\":null,\"Trade\":\"2004-08-12\",\"TreatmentAnimalCertainlyMillion\":\"http://arellano.com/\",\"Whether\":\"Far they brother eat cultural.\",\"against_page_prevent_bring\":\"Australian dollar\",\"arrive_owner_last\":\"2023-01-09\",\"artist\":\"2023-01-09\",\"book_lose_memory_hotel\":\"1.191.245.170\",\"book_special_goal_order\":\"2023-01-09\",\"dog_writer_step_our_certain\":\"143.96.139.141\",\"everything_clear\":\"NAD\",\"food_cultural\":\"c6a7:df6:6912:5e61:c07f:6099:a4e8:4783\",\"history_now_scene_movement_per\":\"2023-01-09\",\"large_growth_little_bit\":\"5739 Luis Ridge Suite 894\\\\nOdomfurt, CO 46210\",\"major_continue_follow_bill_election\":\"Susan Townsend\",\"my_laugh\":\"-71\",\"sport_must_interest\":false}]<eos><pad><pad>'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing collation ---\")\n",
    "val_examples = [debug_data[\"val_dataset\"][i] for i in range(3)]\n",
    "debug_collator_batch(debug_data[\"collator\"], val_examples, debug_data[\"tokenizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36d772bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLLATOR DEBUG ===\n",
      "Input batch size: 3\n",
      "Example 0: 459 tokens, 155 target tokens\n",
      "Example 1: 1038 tokens, 365 target tokens\n",
      "Example 2: 208 tokens, 88 target tokens\n",
      "\n",
      "After collation:\n",
      "Batch shapes: input_ids=torch.Size([3, 1040]), labels=torch.Size([3, 1040])\n",
      "  Example 0: 459 non-pad, 155 non-masked, 459 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"1\":{\"type\":[\"string\",\"null\"]},\"2\":{\"type\":[\"number\",\"null\"]},\"3\":{\"type\":[\"string\",\"null\"]},\"4\":{\"type\":[\"string\",\"null\"]},\"5\":{\"type\":[\"string\",\"null\"]},\"6\":{\"type\":[\"string\",\"null\"]},\"7\":{\"type\":[\"integer\",\"null\"]},\"8\":{\"type\":[\"number\",\"null\"]},\"9\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]\\\\begin{tabular}{lrllllrll}\\n\\\\toprule\\n1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\\\\\\\\n\\\\midrule\\nhttp://www.johnston.info/ & -14.950000 & 124.238.178.178 & LightYellow & -1.0 & Lance Goodman & 1 & NaN & http://evans-gordon.biz/ \\\\\\\\\\nhttps://www.myers-jones.com/ & -47.100000 & 32.152.202.158 & DarkGray & 9.43 & Christine Ramsey & -77 & NaN & https://wu.net/ \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n[OUTPUT][{\"1\":\"http://www.johnston.info/\",\"2\":-14.95,\"3\":\"124.238.178.178\",\"4\":\"LightYellow\",\"5\":\"-1.0\",\"6\":\"Lance Goodman\",\"7\":1,\"8\":null,\"9\":\"http://evans-gordon.biz/\"},{\"1\":\"https://www.myers-jones.com/\",\"2\":-47.1,\"3\":\"32.152.202.158\",\"4\":\"DarkGray\",\"5\":\"9.43\",\"6\":\"Christine Ramsey\",\"7\":-77,\"8\":null,\"9\":\"https://wu.net/\"}]<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Example 1: 1038 non-pad, 365 non-masked, 1038 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"1\":{\"type\":[\"boolean\",\"null\"]},\"10\":{\"type\":[\"string\",\"null\"]},\"11\":{\"type\":[\"string\",\"null\"]},\"12\":{\"type\":[\"number\",\"null\"]},\"13\":{\"type\":[\"string\",\"null\"]},\"2\":{\"type\":[\"string\",\"null\"]},\"3\":{\"type\":[\"string\",\"null\"]},\"4\":{\"type\":[\"string\",\"null\"]},\"5\":{\"type\":[\"boolean\",\"null\"]},\"6\":{\"type\":[\"string\",\"null\"]},\"7\":{\"type\":[\"string\",\"null\"]},\"8\":{\"type\":[\"string\",\"null\"]},\"9\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th>1</th>\\n      <th>2</th>\\n      <th>3</th>\\n      <th>4</th>\\n      <th>5</th>\\n      <th>6</th>\\n      <th>7</th>\\n      <th>8</th>\\n      <th>9</th>\\n      <th>10</th>\\n      <th>11</th>\\n      <th>12</th>\\n      <th>13</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>True</td>\\n      <td>3199 Webb Forest Suite 953\\\\nPopemouth, CT 92732</td>\\n      <td>436.838.2401</td>\\n      <td>Southern star meeting opportunity.</td>\\n      <td>True</td>\\n      <td>1979-01-18</td>\\n      <td>Paraguayan guarani</td>\\n      <td>ETB</td>\\n      <td>spencerbrian@example.org</td>\\n      <td>Brown</td>\\n      <td>-25</td>\\n      <td>None</td>\\n      <td>Guess red difference capital benefit.</td>\\n    </tr>\\n    <tr>\\n      <td>False</td>\\n      <td>55111 Mcdonald Curve Apt. 809\\\\nCraneton, AR 14166</td>\\n      <td>+1-366-733-2676</td>\\n      <td>Congress theory how newspaper hit.</td>\\n      <td>True</td>\\n      <td>1971-02-25</td>\\n      <td>North Korean won</td>\\n      <td>ALL</td>\\n      <td>michaelolsen@example.com</td>\\n      <td>DarkGray</td>\\n      <td>94</td>\\n      <td>None</td>\\n      <td>Business worker country health probably reduce.</td>\\n    </tr>\\n    <tr>\\n      <td>True</td>\\n      <td>928 Herrera Circle\\\\nTheresaland, AL 04882</td>\\n      <td>2508097180</td>\\n      <td>Notice current change.</td>\\n      <td>False</td>\\n      <td>2010-02-09</td>\\n      <td>Aruban florin</td>\\n      <td>LAK</td>\\n      <td>erinsmith@example.org</td>\\n      <td>Cornsilk</td>\\n      <td>74</td>\\n      <td>None</td>\\n      <td>Pm age help fill peace.</td>\\n    </tr>\\n  </tbody>\\n</table>[OUTPUT][{\"1\":true,\"10\":\"Brown\",\"11\":\"-25\",\"12\":null,\"13\":\"Guess red difference capital benefit.\",\"2\":\"3199 Webb Forest Suite 953\\\\nPopemouth, CT 92732\",\"3\":\"436.838.2401\",\"4\":\"Southern star meeting opportunity.\",\"5\":true,\"6\":\"1979-01-18\",\"7\":\"Paraguayan guarani\",\"8\":\"ETB\",\"9\":\"spencerbrian@example.org\"},{\"1\":false,\"10\":\"DarkGray\",\"11\":\"94\",\"12\":null,\"13\":\"Business worker country health probably reduce.\",\"2\":\"55111 Mcdonald Curve Apt. 809\\\\nCraneton, AR 14166\",\"3\":\"+1-366-733-2676\",\"4\":\"Congress theory how newspaper hit.\",\"5\":true,\"6\":\"1971-02-25\",\"7\":\"North Korean won\",\"8\":\"ALL\",\"9\":\"michaelolsen@example.com\"},{\"1\":true,\"10\":\"Cornsilk\",\"11\":\"74\",\"12\":null,\"13\":\"Pm age help fill peace.\",\"2\":\"928 Herrera Circle\\\\nTheresaland, AL 04882\",\"3\":\"2508097180\",\"4\":\"Notice current change.\",\"5\":false,\"6\":\"2010-02-09\",\"7\":\"Aruban florin\",\"8\":\"LAK\",\"9\":\"erinsmith@example.org\"}]<eos><pad><pad>'\n",
      "  Example 2: 208 non-pad, 88 non-masked, 208 attention\n",
      "    Decoded : '<bos>Convert input data to json according to JSONSchema\\n[SCHEMA]{\"items\":{\"properties\":{\"DiscoverTraditional\":{\"type\":[\"number\",\"null\"]},\"DriveSeriesSecondCharacterAgain\":{\"type\":[\"string\",\"null\"]}},\"type\":[\"object\",\"null\"]},\"type\":[\"array\",\"null\"]}[INPUT]DiscoverTraditional,DriveSeriesSecondCharacterAgain\\n-608.15,Accept spring it I.\\n5.469,Someone goal act stage.\\n6.1,Beat understand full minute.\\n568.53,I together want expect.\\n[OUTPUT][{\"DiscoverTraditional\":-608.15,\"DriveSeriesSecondCharacterAgain\":\"Accept spring it I.\"},{\"DiscoverTraditional\":5.469,\"DriveSeriesSecondCharacterAgain\":\"Someone goal act stage.\"},{\"DiscoverTraditional\":6.1,\"DriveSeriesSecondCharacterAgain\":\"Beat understand full minute.\"},{\"DiscoverTraditional\":568.53,\"DriveSeriesSecondCharacterAgain\":\"I together want expect.\"}]<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n"
     ]
    }
   ],
   "source": [
    "train_examples = [debug_data[\"train_dataset\"][i] for i in range(3)]\n",
    "debug_collator_batch(debug_data[\"collator\"], train_examples, debug_data[\"tokenizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'name': {'title': 'Name', 'type': 'string'}, 'urgency': {'enum': ['high', 'medium', 'low'], 'title': 'Urgency', 'type': 'string'}, 'issue': {'title': 'Issue', 'type': 'string'}, 'reporter': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Reporter'}}, 'required': ['name', 'urgency', 'issue', 'reporter'], 'title': 'Customer', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal, Optional\n",
    "import outlines\n",
    "import openai\n",
    "\n",
    "class Customer(BaseModel):\n",
    "    name: str\n",
    "    urgency: Literal[\"high\", \"medium\", \"low\"]\n",
    "    issue: str\n",
    "    reporter: Optional[str]\n",
    "\n",
    "# dump the schema\n",
    "print(Customer.model_json_schema())\n",
    "\n",
    "client = openai.OpenAI()\n",
    "model = outlines.from_openai(client, \"gpt-4o\")\n",
    "\n",
    "customer = model(\n",
    "    \"Alice needs help with login issues ASAP\",\n",
    "    Customer\n",
    ")\n",
    "# ✓ Always returns valid Customer object\n",
    "# ✓ No parsing, no errors, no retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2d18d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"urgency\": {\"enum\": [\"high\", \"medium\", \"low\"], \"title\": \"Urgency\", \"type\": \"string\"}, \"issue\": {\"title\": \"Issue\", \"type\": \"string\"}, \"reporter\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Reporter\"}}, \"required\": [\"name\", \"urgency\", \"issue\", \"reporter\"], \"title\": \"Customer\", \"type\": \"object\"}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(Customer.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebef7737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgrammar.compiler.CompiledGrammar at 0x332ec8dd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgrammar as xgr\n",
    "\n",
    "schema = \"\"\"\n",
    "{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": [\"string\", \"null\"]}}, \"title\": \"Customer\", \"type\": \"object\"}\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_info = xgr.TokenizerInfo.from_huggingface(debug_data['tokenizer'])\n",
    "grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n",
    "compiled_grammar = grammar_compiler.compile_json_schema(schema)\n",
    "compiled_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f034c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m xgr_logits_processor = xgr.contrib.hf.LogitsProcessor(compiled_grammar)\n\u001b[32m      2\u001b[39m generated_ids = model.generate(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     **\u001b[43mmodel_inputs\u001b[49m, max_new_tokens=\u001b[32m512\u001b[39m, logits_processor=[xgr_logits_processor]\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m generated_ids = generated_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(model_inputs.input_ids[\u001b[32m0\u001b[39m]) :]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'model_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# xgr_logits_processor = xgr.contrib.hf.LogitsProcessor(compiled_grammar)\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs, max_new_tokens=512, logits_processor=[xgr_logits_processor]\n",
    "# )\n",
    "# generated_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n",
    "# print(tokenizer.decode(generated_ids, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
